{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EmotionDataset(data.Dataset):\n",
    "    def __init__(self, root_dir, challenge, sequence_length=10, window_size=10):\n",
    "        self.root_dir = root_dir\n",
    "        self.challenge = challenge\n",
    "        self.sequence_length = sequence_length\n",
    "        self.window_size = window_size\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load data\n",
    "        for file in tqdm(os.listdir(root_dir), desc=f\"Loading {challenge} dataset\"):\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root_dir, file)\n",
    "                try:\n",
    "                    features = np.loadtxt(file_path, delimiter=\",\", dtype=np.float32)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error loading {file}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                if features.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                if len(features.shape) == 1:\n",
    "                    features = features.reshape(1, -1)\n",
    "                \n",
    "                if self.challenge == 'VA':\n",
    "                    labels = features[:, -2:]\n",
    "                elif self.challenge == 'EXPR':\n",
    "                    labels = features[:, -1].astype(int)\n",
    "                elif self.challenge == 'AU':\n",
    "                    labels = features[:, -12:]\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid challenge type\")\n",
    "                \n",
    "                # Filtering conditions\n",
    "                filtered_features = []\n",
    "                filtered_labels = []\n",
    "                for i, label in enumerate(labels):\n",
    "                    if self.challenge == 'VA' and (-5 in label):\n",
    "                        continue\n",
    "                    if self.challenge == 'EXPR' and label == -1:\n",
    "                        continue\n",
    "                    if self.challenge == 'AU' and (-1 in label):\n",
    "                        continue\n",
    "                    if self.challenge == 'EXPR':\n",
    "                        filtered_features.append(features[i, :-1])\n",
    "                        filtered_labels.append(label)\n",
    "                    else:\n",
    "                        filtered_features.append(features[i, :-len(label)])\n",
    "                        filtered_labels.append(label)\n",
    "                if filtered_features:\n",
    "                    self.data.append(np.array(filtered_features))\n",
    "                    self.labels.append(np.array(filtered_labels))\n",
    "        \n",
    "        \n",
    "\n",
    "        # For EXPR challenge, convert labels to one-hot encoding\n",
    "        if self.challenge == 'EXPR':\n",
    "            self.labels = [np.eye(8)[label] for label in self.labels]\n",
    "\n",
    "        # Convert the lists to numpy arrays after filtering\n",
    "        self.data = np.vstack(self.data) if self.data else np.array([])\n",
    "        self.labels = np.vstack(self.labels) if self.labels else np.array([])\n",
    "        \n",
    "        # Create sequences with temporal coherence using window size and sequence length\n",
    "        self.sequences = []\n",
    "        self.sequence_labels = []\n",
    "        \n",
    "        for i in range(0, len(self.data) - self.sequence_length + 1, self.window_size):\n",
    "            sequence_data = self.data[i:i+self.sequence_length]\n",
    "            sequence_label = self.labels[i:i+self.sequence_length]  # Capture labels for all frames in sequence\n",
    "            self.sequences.append(sequence_data)\n",
    "            self.sequence_labels.append(sequence_label)\n",
    "        \n",
    "        self.sequences = np.array(self.sequences)\n",
    "        self.sequence_labels = np.array(self.sequence_labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.sequences[idx], dtype=torch.float32),\n",
    "                torch.tensor(self.sequence_labels[idx], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = \"EXPR\"  # Emotion classification\n",
    "\n",
    "# Load dataset and create data loaders\n",
    "train_set = EmotionDataset(f'Features_CLIP/{challenge}/training_set_features', challenge, sequence_length=100, window_size=20)\n",
    "val_set = EmotionDataset(f'Features_CLIP/{challenge}/validation_set_features', challenge, sequence_length=100,window_size=100)\n",
    "    \n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_descriptions = {\n",
    "    \"Neutral\": \"A face showing neutrality.\",\n",
    "    \"Anger\": \"A face showing anger.\",\n",
    "    \"Disgust\": \"A face showing disgust.\",\n",
    "    \"Fear\": \"A face showing fear.\",\n",
    "    \"Happiness\": \"A face showing happiness.\",\n",
    "    \"Sadness\": \"A face showing sadness.\",\n",
    "    \"Surprise\": \"A face showing surprise.\",\n",
    "    \"Other\": \"A face showing an undefined emotion.\"\n",
    "}\n",
    "\n",
    "\n",
    "import open_clip\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-16\", pretrained=\"openai\")\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Tokenize text descriptions in smaller chunks\n",
    "emotion_texts = list(emotion_descriptions.values())\n",
    "\n",
    "# Process each text separately to avoid memory issues\n",
    "text_features = []\n",
    "with torch.no_grad():\n",
    "    for text in emotion_texts:\n",
    "        tokenized = tokenizer([text]).to(device)  # Tokenize one at a time\n",
    "        feature = model.encode_text(tokenized)\n",
    "        feature /= feature.norm(dim=-1, keepdim=True)  # Normalize\n",
    "        text_features.append(feature.cpu())  # Move to CPU to free up GPU memory\n",
    "\n",
    "text_features = torch.vstack(text_features).to(device)  # Move back to GPU after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CLIP_E_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=512, num_layers=1):\n",
    "        super(CLIP_E_LSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 512)  # Project LSTM output to feature space\n",
    "    \n",
    "    def forward(self, img_features):\n",
    "        # LSTM expects (batch_size, sequence_length, feature_dim)\n",
    "        lstm_out, _ = self.lstm(img_features)  # Output shape: (batch, seq_len, hidden_dim)\n",
    "        return self.fc(lstm_out)  # Take last LSTM output (seq-to-single)\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, caption_embeddings, image_embeddings):\n",
    "        \"\"\"\n",
    "        Compute contrastive loss based on normalized embeddings.\n",
    "        \n",
    "        Args:\n",
    "            caption_embeddings (torch.Tensor): (N, D) tensor with text embeddings.\n",
    "            image_embeddings (torch.Tensor): (N, D) tensor with image embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Scalar contrastive loss.\n",
    "        \"\"\"\n",
    "        # Normalize embeddings\n",
    "        #print(np.shape(caption_embeddings))\n",
    "        image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "        \n",
    "        caption_embeddings = F.normalize(caption_embeddings, p=2, dim=-1)\n",
    "        #print(np.shape(caption_embeddings))\n",
    "        # Compute cosine similarities (logits)\n",
    "        logits = (caption_embeddings @ image_embeddings.T) / self.temperature\n",
    "        \n",
    "        # Compute similarities within images and within captions\n",
    "        \n",
    "        images_similarity = (image_embeddings @ image_embeddings.T)\n",
    "        captions_similarity = (caption_embeddings @ caption_embeddings.T)\n",
    "\n",
    "        # Compute targets as the softmax of the mean similarity\n",
    "        targets = F.softmax((captions_similarity + images_similarity) / (2 * self.temperature), dim=-1)\n",
    "\n",
    "        # Compute loss for captions and images\n",
    "        captions_loss = F.cross_entropy(logits, targets, reduction=\"mean\")\n",
    "        images_loss = F.cross_entropy(logits.T, targets.T, reduction=\"mean\")\n",
    "\n",
    "        # Return the mean loss\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "def train_clip_e(model, train_loader, text_features, optimizer, device, epochs=10):\n",
    "    model.train()\n",
    "    contrastive_loss_fn = ContrastiveLoss().to(device)  # Instantiate the loss function\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for img_features, labels in train_loader:\n",
    "         \n",
    "            img_features = img_features.to(device)  # Shape: (batch, seq_len, 512)\n",
    "            labels = labels.argmax(dim=2).to(device)  # Convert one-hot to class indices\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            img_features = model(img_features)  # Process through LSTM\n",
    "            \n",
    "            batch_text_features = text_features[labels].to(device)\n",
    "            # Joining dimensions 0 and 1\n",
    "            batch_text_features = batch_text_features.view(-1, batch_text_features.size(2))  \n",
    "            img_features = img_features.view(-1, img_features.size(2))\n",
    "            \n",
    "           \n",
    "            loss = contrastive_loss_fn(batch_text_features, img_features)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        evaluate_clip_e(clip_e_model, val_loader, text_features, device)\n",
    "\n",
    "# Inference Function (Sequence-based)\n",
    "def infer_clip_e(model, img_features, text_features):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        img_features = model(img_features)  \n",
    "        \n",
    "            # Normalize image features\n",
    "        # Normalize image features (batch_size, seq_len, feature_dim)\n",
    "        img_features = F.normalize(img_features, p=2, dim=-1)  # Normalize image features\n",
    "        \n",
    "        # Normalize text features (8, feature_dim)\n",
    "        text_features = F.normalize(text_features, p=2, dim=-1)  # Normalize text features\n",
    "        \n",
    "        # Reshape image features by joining the first two dimensions (batch_size * seq_len, feature_dim)\n",
    "        img_features_reshaped = img_features.view(-1, img_features.size(-1))  # (batch_size * seq_len, feature_dim)\n",
    "\n",
    "        # Compute similarity (batch_size * seq_len, 8) using batch-wise matrix multiplication\n",
    "        probs = torch.einsum('qd,td->qt', img_features_reshaped, text_features)  # (batch_size * seq_len, 8)\n",
    "\n",
    "\n",
    "        probs = F.softmax(probs, dim=-1)  # (batch_size * seq_len, 8)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Find the most similar text feature index for each image feature\n",
    "        final_pred = torch.argmax(probs, dim=-1)  # (batch_size, seq_len)\n",
    "     \n",
    "    \n",
    "\n",
    "        \n",
    "        return final_pred\n",
    "from sklearn.metrics import classification_report\n",
    "# Evaluation Function (F1-Score Tracking)\n",
    "def evaluate_clip_e(model, data_loader, text_features, device, best_f1=0.0, save_path=\"FIXED_EXPR_CLIP_temp_new.pt\"):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_features, labels in data_loader:\n",
    "            img_features = img_features.to(device)\n",
    "            labels = labels.argmax(dim=2).to(device)\n",
    "            #img_features = img_features.view(-1, img_features.size(2))\n",
    "\n",
    "            preds = infer_clip_e(model, img_features, text_features)\n",
    "\n",
    "            # Join dimensions 0 and 1 to create a 1D tensor\n",
    "            labels = labels.view(-1)  # Shape will be (64 * 8,)\n",
    "  \n",
    "            \n",
    "            preds = preds.view(-1) \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "    \n",
    "    for emotion, metrics in report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            print(f\"Emotion {emotion}: F1-Score {metrics['f1-score']:.4f}\")\n",
    "    \n",
    "    macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "    print(f\"Overall Macro F1-score: {macro_f1:.4f}\")\n",
    "\n",
    "    #plt.hist(all_preds,bins=20, color='blue', edgecolor='black')\n",
    "    #plt.show()\n",
    "\n",
    "    #plt.hist(all_labels,bins=20, color='blue', edgecolor='black')\n",
    "    #plt.show()\n",
    "\n",
    "    # Save model if it's the best so far\n",
    "    \n",
    "    if macro_f1 > 0.33:\n",
    "        best_f1 = macro_f1\n",
    "        print([macro_f1,best_f1])\n",
    "        # Save model state_dict\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        print(f\"New best model saved with Macro F1-score: {macro_f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    return macro_f1, best_f1\n",
    "# 🔹 Initialize and Train\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_e_model = CLIP_E_LSTM().to(device)\n",
    "optimizer = optim.Adam(clip_e_model.parameters(), lr=1e-5)\n",
    "\n",
    "train_clip_e(clip_e_model, train_loader, text_features, optimizer, device, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
