{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.utils.data as data\n",
    "from networks.DDAM_ABAW import DDAMNet\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = DDAMNet(num_class=8, num_head=2, pretrained=False, train_val_arousal=True, train_emotions=False, train_actions=False)\n",
    "model_path = 'checkpoints_ver2.0/affecnet8_epoch25_acc0.6469.pth'\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")  # Load in CPU first to avoid GPU memory spike\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "model.eval()\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "root_dir = \"../8th_ABAW\"  # Replace with actual path (batch1, batch2)\n",
    "label_dir = \"6th_ABAW_Annotations\"  # Replace with actual path\n",
    "\n",
    "# Define transformations (resize, normalize, convert to tensor)\n",
    "transform = T.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def process_video(video_name, label_file_path, output_dir, model, root_dir, challenge, device, transform):\n",
    "    try:\n",
    "        # Locate the video folder in batch1 or batch2\n",
    "        video_folder = None\n",
    "        for batch in ['batch1', 'batch2']:\n",
    "            possible_path = os.path.join(root_dir, batch, 'cropped_aligned', video_name)\n",
    "            if os.path.exists(possible_path):\n",
    "                video_folder = possible_path\n",
    "                break\n",
    "\n",
    "        if video_folder is None or not os.path.exists(video_folder):\n",
    "            print(f\"Skipping {video_name}: video folder not found.\")\n",
    "            return\n",
    "\n",
    "        # Read label file\n",
    "        with open(label_file_path, 'r') as file:\n",
    "            lines = file.readlines()[1:]  # Skip header row\n",
    "\n",
    "        # Get sorted list of frame paths\n",
    "        frame_paths = sorted([os.path.join(video_folder, frame) for frame in os.listdir(video_folder) if frame.endswith('.jpg')])\n",
    "\n",
    "        # Process frames in batches\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        # Output file path\n",
    "        output_file = os.path.join(output_dir, f\"{video_name}.txt\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for frame_path, label in zip(frame_paths, lines):\n",
    "                frame_label = list(map(float, label.strip().split(',')))\n",
    "\n",
    "                # Load and preprocess the image\n",
    "                image = Image.open(frame_path).convert('RGB')\n",
    "                image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                # Append image and label to the batch\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(frame_label)\n",
    "\n",
    "                # Process the batch if it's full\n",
    "                if len(batch_images) == 32:\n",
    "                    # Stack the batch together\n",
    "                    batch_images_tensor = torch.cat(batch_images, dim=0)\n",
    "\n",
    "                    # Extract features\n",
    "                    with torch.no_grad():\n",
    "                        features = model(batch_images_tensor)  # Shape: (batch_size, 512, 7, 7)\n",
    "\n",
    "                    # Check if features are a tuple\n",
    "                    if isinstance(features, tuple):\n",
    "                        features = features[3]  # Extract the first element (assuming it's the features)\n",
    "\n",
    "                    # Process each frame in the batch and write to file\n",
    "                    for i in range(len(batch_images)):\n",
    "                        feature_vector = features[i].flatten().cpu().numpy()  # Flatten the feature map to a vector\n",
    "                        label_vector = batch_labels[i]\n",
    "\n",
    "                        # Write features and labels as a single line in the txt file\n",
    "                        line = ','.join(map(str, feature_vector)) + ',' + ','.join(map(str, label_vector)) + '\\n'\n",
    "                        f.write(line)\n",
    "\n",
    "                    # Clear the batch for the next set of images\n",
    "                    batch_images = []\n",
    "                    batch_labels = []\n",
    "\n",
    "            # If there are any remaining frames after the loop\n",
    "            if len(batch_images) > 0:\n",
    "                batch_images_tensor = torch.cat(batch_images, dim=0)\n",
    "                with torch.no_grad():\n",
    "                    features = model(batch_images_tensor)\n",
    "\n",
    "                if isinstance(features, tuple):\n",
    "                    features = features[3]  # Extract the first element (assuming it's the features)\n",
    "\n",
    "                for i in range(len(batch_images)):\n",
    "                    feature_vector = features[i].flatten().cpu().numpy()\n",
    "                    label_vector = batch_labels[i]\n",
    "\n",
    "                    # Write remaining features and labels to file\n",
    "                    line = ','.join(map(str, feature_vector)) + ',' + ','.join(map(str, label_vector)) + '\\n'\n",
    "                    f.write(line)\n",
    "\n",
    "        print(f\"Processed {video_name} -> {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_name}: {str(e)}\")\n",
    "\n",
    "def extract_and_save_features(model, root_dir, label_dir, challenge, output_root):\n",
    "    \"\"\"\n",
    "    Extracts features from video frames and saves them as Pickle (.pkl) files.\n",
    "    \"\"\"\n",
    "    # Validate challenge type\n",
    "    challenge_folders = {'AU': 'AU_Detection_Challenge', 'EXPR': 'EXPR_Recognition_Challenge', 'VA': 'VA_Estimation_Challenge'}\n",
    "    if challenge not in challenge_folders:\n",
    "        raise ValueError(f\"Invalid challenge: {challenge}. Must be 'AU', 'EXPR', or 'VA'.\")\n",
    "\n",
    "    challenge_folder = challenge_folders[challenge]\n",
    "\n",
    "    # Define label folder paths\n",
    "    train_label_folder = os.path.join(label_dir, challenge_folder, 'Train_Set')\n",
    "    val_label_folder = os.path.join(label_dir, challenge_folder, 'Validation_Set')\n",
    "\n",
    "    # Define output directories\n",
    "    output_challenge_dir = os.path.join(output_root, challenge)  # e.g., output_root/AU/\n",
    "    train_output_dir = os.path.join(output_challenge_dir, 'training_set_features')\n",
    "    val_output_dir = os.path.join(output_challenge_dir, 'validation_set_features')\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(train_output_dir, exist_ok=True)\n",
    "    os.makedirs(val_output_dir, exist_ok=True)\n",
    "\n",
    "    # Find label files\n",
    "    label_files = []\n",
    "    for folder, output_dir in [(train_label_folder, train_output_dir), (val_label_folder, val_output_dir)]:\n",
    "        if os.path.exists(folder):\n",
    "            label_files += [(folder, output_dir, f) for f in os.listdir(folder) if f.endswith('.txt')]\n",
    "\n",
    "    if not label_files:\n",
    "        raise FileNotFoundError(f\"No label files found in {train_label_folder} or {val_label_folder}.\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Handle outliers based on challenge type\n",
    "    if challenge == 'VA':\n",
    "        model = DDAMNet(num_class=8, num_head=2, pretrained=False, train_val_arousal=True, train_emotions=False, train_actions=False)\n",
    "        print(\"Model initialized for VA challenge\")\n",
    "\n",
    "    if challenge == 'EXPR':\n",
    "        model = DDAMNet(num_class=8, num_head=2, pretrained=False, train_val_arousal=False, train_emotions=True, train_actions=False)\n",
    "        print(\"Model initialized for EXPR challenge\")\n",
    "\n",
    "    if challenge == 'AU':\n",
    "        model = DDAMNet(num_class=8, num_head=2, pretrained=False, train_val_arousal=False, train_emotions=False, train_actions=True)\n",
    "        print(\"Model initialized for AU challenge\")\n",
    "    model_path = 'checkpoints_ver2.0/affecnet8_epoch25_acc0.6469.pth'\n",
    "    checkpoint = torch.load(model_path, map_location=\"cpu\")  # Load in CPU first to avoid GPU memory spike\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Define transformations (resize, normalize, convert to tensor)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    total_videos = len(label_files)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        with tqdm(total=total_videos, desc=\"Processing videos\") as pbar:\n",
    "            for label_folder, output_dir, label_file in label_files:\n",
    "                video_name = label_file.replace('.txt', '')\n",
    "                label_file_path = os.path.join(label_folder, label_file)\n",
    "                futures.append(executor.submit(process_video, video_name, label_file_path, output_dir, model, root_dir, challenge, device, transform))\n",
    "\n",
    "            # Wait for all futures to complete\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "    print(f\"Feature extraction completed for {challenge}. Files saved in {output_challenge_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenges = [\"VA\",\"EXPR\",\"AU\"]    \n",
    "for i in challenges:\n",
    "    extract_and_save_features(\n",
    "        model=model,  # Your pre-trained model\n",
    "        root_dir=root_dir,  # Root dataset directory\n",
    "        label_dir=label_dir,  # Label directory\n",
    "        challenge=i,  # 'AU', 'EXPR', or 'VA'\n",
    "        output_root=\"Features\"  # Where to save extracted features\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
